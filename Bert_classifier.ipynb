{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aeff8866-26a0-47c1-b637-75e92473b9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, dataset_dict, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1f41cd6-75fa-4d48-a44f-13b0eaa585b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"C:/rubert-tiny2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0897b184-7b1a-4d4b-8202-1ba0de007160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='C:/rubert-tiny2/', vocab_size=83828, model_max_len=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4f3e610-b9aa-40f1-b87b-874887e83244",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"C:/data/ceo-text/full_df_v2.52-21-12-2021.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee76434d-9f9f-40dc-8a9f-d0dabe5ebd54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Комментарий</th>\n",
       "      <th>target</th>\n",
       "      <th>sub_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>со слов клиента- банкомат не однократно осущес...</td>\n",
       "      <td>cards</td>\n",
       "      <td>cards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>добрый день!просьба удаленно перезагрузить ус....</td>\n",
       "      <td>remote_reboot,cassette</td>\n",
       "      <td>remote_reboot,cassette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>нет операций</td>\n",
       "      <td>no_oper</td>\n",
       "      <td>no_oper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>просьба перезагрузить банкомат по питанию удал...</td>\n",
       "      <td>remote_reboot</td>\n",
       "      <td>remote_reboot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>проблемы с картридером - захват карт - не возв...</td>\n",
       "      <td>cards</td>\n",
       "      <td>cards</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Комментарий                  target  \\\n",
       "0  со слов клиента- банкомат не однократно осущес...                   cards   \n",
       "1  добрый день!просьба удаленно перезагрузить ус....  remote_reboot,cassette   \n",
       "2                                       нет операций                 no_oper   \n",
       "3  просьба перезагрузить банкомат по питанию удал...           remote_reboot   \n",
       "4  проблемы с картридером - захват карт - не возв...                   cards   \n",
       "\n",
       "               sub_target  \n",
       "0                   cards  \n",
       "1  remote_reboot,cassette  \n",
       "2                 no_oper  \n",
       "3           remote_reboot  \n",
       "4                   cards  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a172f76b-8338-4739-9053-e6e1aa983914",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.target == 'no_oper') | (df.target == 'cards') | (df.target == 'remote_reboot') | (df.target == 'host_close')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7cddf9b8-a599-480e-b592-170b92a13061",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df.target.factorize()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a49c08d0-8c03-478a-9e9b-5dfdf5de1a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Комментарий</th>\n",
       "      <th>target</th>\n",
       "      <th>sub_target</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>со слов клиента- банкомат не однократно осущес...</td>\n",
       "      <td>cards</td>\n",
       "      <td>cards</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>нет операций</td>\n",
       "      <td>no_oper</td>\n",
       "      <td>no_oper</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>просьба перезагрузить банкомат по питанию удал...</td>\n",
       "      <td>remote_reboot</td>\n",
       "      <td>remote_reboot</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>проблемы с картридером - захват карт - не возв...</td>\n",
       "      <td>cards</td>\n",
       "      <td>cards</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>отсутствуют операции</td>\n",
       "      <td>no_oper</td>\n",
       "      <td>no_oper</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Комментарий         target  \\\n",
       "0  со слов клиента- банкомат не однократно осущес...          cards   \n",
       "2                                       нет операций        no_oper   \n",
       "3  просьба перезагрузить банкомат по питанию удал...  remote_reboot   \n",
       "4  проблемы с картридером - захват карт - не возв...          cards   \n",
       "5                               отсутствуют операции        no_oper   \n",
       "\n",
       "      sub_target  label  \n",
       "0          cards      0  \n",
       "2        no_oper      1  \n",
       "3  remote_reboot      2  \n",
       "4          cards      0  \n",
       "5        no_oper      1  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5db4e5c8-f6fd-4b84-ab0e-c5467fd7fcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df['Комментарий'], df['label'], test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "94af4ce4-f595-4870-a149-40e9be089008",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1a5433bc-090d-4f13-9181-7f78a84ac611",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CeoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2a64b8af-3749-4265-bb98-149626eb8514",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CeoDataset(train_encodings, train_labels.tolist())\n",
    "val_dataset = CeoDataset(val_encodings, val_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eed2c0b1-ef7f-4035-8ea8-ade7af3446fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f6faf9b0-8c4f-461a-8f9b-d83afdd5f133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file C:/rubert-tiny2/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"C:/rubert-tiny2/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"emb_size\": 312,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 312,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 600,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 83828\n",
      "}\n",
      "\n",
      "loading weights file C:/rubert-tiny2/pytorch_model.bin\n",
      "Some weights of the model checkpoint at C:/rubert-tiny2/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at C:/rubert-tiny2/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"C:/rubert-tiny2/\", num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fb49fe-68bc-46eb-8a51-5b850eedf5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "31e64965-7c06-4edd-98d5-657538c4961f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "93ceb8a2-71ff-4962-b0c5-eb483046426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics = compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "71d665d1-ea75-46c0-8248-bdd0f0be60af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Алексей\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 14107\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4410\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4410' max='4410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4410/4410 18:15:28, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.282800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.034500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.017700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.009600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.008200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.006500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-1000\n",
      "Configuration saved in ./results\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-1000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-1500\n",
      "Configuration saved in ./results\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-1500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-2000\n",
      "Configuration saved in ./results\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-2000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-2500\n",
      "Configuration saved in ./results\\checkpoint-2500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-2500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-3000\n",
      "Configuration saved in ./results\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-3000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-3500\n",
      "Configuration saved in ./results\\checkpoint-3500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-3500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-3500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-3500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-4000\n",
      "Configuration saved in ./results\\checkpoint-4000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-4000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-4000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-4000\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4410, training_loss=0.04413772018588319, metrics={'train_runtime': 65730.1202, 'train_samples_per_second': 1.073, 'train_steps_per_second': 0.067, 'total_flos': 183925459494840.0, 'train_loss': 0.04413772018588319, 'epoch': 5.0})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "23355219-f2d3-409e-983a-d7618f1cf024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2f4219d8-4bfb-41cd-9017-dbb1c7e948d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in C:\\model.json\\config.json\n",
      "Model weights saved in C:\\model.json\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"C:\\\\model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "327154c4-5a08-4077-bcbe-8d36fdec9fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file C:\\model.json\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"C:\\\\model.json\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"emb_size\": 312,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 312,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 600,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 83828\n",
      "}\n",
      "\n",
      "loading weights file C:\\model.json\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at C:\\model.json.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#|tokenizer = AutoTokenizer.from_pretrained(r\"C:\\model.json\\\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"C:\\model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a078f842-5836-460d-b3d4-260a478a580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = pipeline(task='text-classification', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "92dfe0ea-ece5-4b01-a333-a58422f81e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_3', 'score': 0.9987146854400635}]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf(\"ЗАкрыть на хосте\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "674f76ab-8825-4f9b-b82c-f447be0bb910",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Комментарий', 'target']].rename(columns={'Комментарий':'text','target': 'labels'}).to_csv('C:/data/ceo-text/ceo_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c4969f72-b6b0-41e6-8c4d-ab85aa39c9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-70dfc129d4b6391b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:\\Users\\Алексей\\.cache\\huggingface\\datasets\\csv\\default-70dfc129d4b6391b\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Extracting data files: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:\\Users\\Алексей\\.cache\\huggingface\\datasets\\csv\\default-70dfc129d4b6391b\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 124.96it/s]\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset('csv', data_files='C:/data/ceo-text/ceo_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1ace09fd-d2db-4bc2-9b67-795528399743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 20153\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0595d66a-6c17-4475-a356-47e8c6202ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    return tokenizer(data['text'], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "06b3c2eb-d090-481c-9e15-1ac0f820fd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (C:\\Users\\Алексей\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 53.58it/s]\n"
     ]
    }
   ],
   "source": [
    "imdb = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38da30ec-0053-4031-838a-a3f52bd37ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "71267512-afaf-457b-b619-9b1faaebf676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO написать функцию, которая преобразует датасет CEO в формат transformers\n",
    "# Возможно, нужно будет еще labels перевести в инты\n",
    "# TODO нужен препрцессинг текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4afd0f5e-0ff0-446e-be92-dd727aa75287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 21/21 [00:00<00:00, 28.86ba/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:10<00:00,  2.46ba/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.46ba/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:15<00:00,  3.22ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = data.map(preprocess, batched=True)\n",
    "tokenized_df = imdb.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "89643a10-0354-4e42-991d-7e79db35f8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 20153\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ebf93415-f4bf-49c2-9d2f-f09c3d976391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f7cc3113-d06c-4dd0-b89c-d680678fe952",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1e1ec022-f1cc-4ef6-b287-c279d58e6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7d726e07-d81f-4066-bc5e-de4a6f912f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file C:/rubert-tiny2/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"C:/rubert-tiny2/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"emb_size\": 312,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 312,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 600,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 83828\n",
      "}\n",
      "\n",
      "loading weights file C:/rubert-tiny2/pytorch_model.bin\n",
      "Some weights of the model checkpoint at C:/rubert-tiny2/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at C:/rubert-tiny2/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"C:/rubert-tiny2/\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "28fbbb3c-2053-4f31-a3a8-ae45de8b1dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "79fd94fe-82ab-4c55-a77d-8268c322419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_df['train'],\n",
    "    eval_dataset=tokenized_df['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f8f537-a07c-408f-af3a-3c79c4112407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\Алексей\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 25000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7815\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1761' max='7815' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1761/7815 12:47:40 < 44:02:09, 0.04 it/s, Epoch 1.13/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.505600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.381500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.345000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-1000\n",
      "Configuration saved in ./results\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-1000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-1500\n",
      "Configuration saved in ./results\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-1500\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7f8ad6-1553-4baf-9d38-5200a9fb8b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
